{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "import cv2\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "from rq import Queue\n",
    "from redis import Redis\n",
    "import PIL\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import imageio\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from ga_model import *\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "\n",
    "import gym.spaces\n",
    "import gym_Rubiks_Cube\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "x = [5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 4, 1, 1, 4, 1, 1, 4, 3, 0, 0, 3, 0, 0, 3, 0, 0, 1,\n",
    " 1, 1, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0]\n",
    "print(len(x) - get_fitness((x, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$CONDA_DEFAULT_ENV\n",
      "['CubeCrash-v0', 'CubeCrashSparse-v0', 'CubeCrashScreenBecomesBlack-v0', 'RubiksCube-v0', 'RubiksCube2x2-v0']\n"
     ]
    }
   ],
   "source": [
    "!echo $CONDA_DEFAULT_ENV\n",
    "# !pip3 install -e .\n",
    "from gym import envs\n",
    "# from gym.envs.registration import register\n",
    "# register(\n",
    "#     id='RubiksCube-v0',\n",
    "#     entry_point='gym_Rubiks_Cube.envs:RubiksCubeEnv'\n",
    "# )\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "cube_env_ids = [env_spec.id for env_spec in all_envs if 'Cube' in env_spec.id]\n",
    "print(cube_env_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('RubiksCube-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(12)\n",
      "[5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 3\n",
      " 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT0AAAD4CAYAAACeyTEuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAL9ElEQVR4nO3db4hld33H8fenu1k0UYniH9Ld0CQQ0qaCJh1S7UJpE5G1ivFBCwkoIsL2gbZJESQWSttnfVDEPJDCsokGTBMkRhokjQY1BKFunfxRk2yk6VbdMasbsZrYB01Xv30wN2Xczs49d+f+7pm7v/cLhrn3zpnhM2d3P/s7995zvqkqJKkXvzZ2AElaJEtPUlcsPUldsfQkdcXSk9SV3S1+aBJfEt7KRWMHWAInxg6gZVdV2ezxJqWnKf507ABL4G/GDqBzlYe3krpi6UnqiqUnqSuWnqSuWHqSumLpSeqKpSepK4NKL8mBJN9J8kySW1qHkqRWppZekl3AJ4F3AFcCNya5snUwSWphyErvGuCZqjpWVS8CdwPXt40lSW0MKb29wPEN99cmj/2KJAeTrCZZnVc4SZq3IefebnbS7v+7oEBVHQIOgRcckLRzDVnprQEXb7i/D3i2TRxJamtI6X0DuDzJpUn2ADcA97WNJUltTD28rapTST4MfBHYBdxeVU82TyZJDQy6nl5V3Q/c3ziLJDXnGRmSumLpSeqKpSepK5aepK5YepK6YulJ6oqlJ6krlp6krlh6krpi6UnqiqUnqSuWnqSuWHqSumLpSeqKpSepK0NGQN6e5GSSJxYRSJJaGrLS+zRwoHEOSVqIqaVXVQ8DP1lAFklqzuf0JHVl0IyMIZIcBA7O6+dJUgtzKz2HfUtaBh7eSurKkLes3AX8C3BFkrUkH2wfS5LaGDLs+8ZFBJGkRfDwVlJXLD1JXbH0JHXF0pPUFUtPUlcsPUldsfQkdcXSk9QVS09SVyw9SV2x9CR1xdKT1BVLT1JXLD1JXbH0JHXF0pPUlSFXTr44yVeTHE3yZJKbFhFMkloYMhjoFPCRqno0ySuBR5I8WFVPNc4mSXM3ZNj3iap6dHL7BeAosLd1MElqYaYRkEkuAa4CjmzyNefeStrxBpdeklcAnwNurqrnT/+6c28lLYNBr94mOY/1wruzqu5tG0mS2hny6m2A24CjVfXx9pEkqZ0hK739wPuAa5M8Pvn4o8a5JKmJIcO+vwZkAVkkqTnPyJDUFUtPUlcsPUldsfQkdcXSk9QVS09SV1I1/zPGVpJanftPPYc02OfnHN8ktbVyB21lZQVWVzffSa70JHXF0pPUFUtPUlcsPUldsfQkdcXSk9QVS09SVyw9SV0ZcuXklyX51yTfnMy9/dtFBJOkFoYMBvpv4Nqq+vlkVsbXkvxzVX29cTZJmrshV04u4OeTu+dNPjyPStJSGjoNbVeSx4GTwINVtenc2ySrSVafm3dKSZqTQaVXVb+oqjcD+4Brkrxxk20OVdVKVa28bt4pJWlOZnr1tqp+CjwEHGiSRpIaG/Lq7euSXDi5/XLgbcDTrYNJUgtDXr29CLgjyS7WS/KzVfWFtrEkqY0hr95+C7hqAVkkqTnPyJDUFUtPUlcsPUldsfQkdcXSk9QVS09SVyw9SV2x9CR1xdKT1BVLT1JXLD1JXbH0JHXF0pPUFUtPUlcsPUldGVx6k+FAjyXxAqKSltYsK72bgKOtgkjSIgwdAbkPeCdwuG0cSWpr6ErvE8BHgV+eaQPn3kpaBkOmob0LOFlVj2y1nXNvJS2DISu9/cC7k3wXuBu4NslnmqaSpEamll5Vfayq9lXVJcANwFeq6r3Nk0lSA75PT1JXhgz7/j9V9RDwUJMkkrQArvQkdcXSk9QVS09SVyw9SV2x9CR1xdKT1BVLT1JXLD1JXbH0JHXF0pPUFUtPUlcsPUldsfQkdcXSk9QVS09SVwZdT29yqfgXgF8Ap6pqpWUoSWpllouI/mFV/bhZEklaAA9vJXVlaOkV8KUkjyQ5uNkGzr2VtAyGHt7ur6pnk7weeDDJ01X18MYNquoQcAhgJak555SkuRi00quqZyefTwKfB65pGUqSWplaekkuSPLKl24DbweeaB1MkloYcnj7BuDzSV7a/h+r6oGmqSSpkamlV1XHgDctIIskNedbViR1xdKT1BVLT1JXLD1JXbH0JHXF0pPUlVTN/4yxeBraFO6e6TJ2AC25qtr0L5ErPUldsfQkdcXSk9QVS09SVyw9SV2x9CR1xdKT1BVLT1JXBpVekguT3JPk6SRHk7y1dTBJamHoYKBbgQeq6o+T7AHOb5hJkpqZehpaklcB3wQuq4HnrHka2jTunuk8DU3bs53T0C4DngM+leSxJIcnA4J+xca5t9vMKknNDFnprQBfZ3327ZEktwLPV9VfbfE9LmW25O6ZzpWetmc7K701YK2qjkzu3wNcPa9gkrRIU0uvqn4IHE9yxeSh64CnmqaSpEYGXU8vyZuBw8Ae4Bjwgar6zy229/htS+6e6Ty81fac6fDWi4iOwt0znaWn7fEiopKEpSepM5aepK5YepK6YulJ6oqlJ6krlp6krlh6krpi6UnqiqUnqSuWnqSuWHqSumLpSeqKpSepK5aepK5MLb0kVyR5fMPH80luXkQ4SZq3mS4immQX8APgd6vqe1ts51Uyt+Tumc6LiGp75nUR0euAf9+q8CRpJ5u19G4A7moRRJIWYfDhbZI9wLPAb1fVjzb5+kHg4OTu78wt4TnJw9vpPLzV9mx7MFCS64EPVdXbB2zrv+otuXums/S0PfN4Tu9GPLSVtOSGzr09HzgOXFZVPxuwvUuZLbl7pnOlp+1x7u2O4u6ZztLT9jj3VpKw9CR1xtKT1BVLT1JXLD1JXbH0JHXF0pPUFUtPUlcsPUldsfQkdcXSk9QVS09SVyw9SV2x9CR1xdKT1BVLT1JXBpVekr9I8mSSJ5LcleRlrYNJUgtTSy/JXuDPgZWqeiOwi/VRkJK0dIYe3u4GXp5kN3A+66MgJWnpTC29qvoB8PfA94ETwM+q6kunb5fkYJLVJKvzjylJ8zHk8PbVwPXApcCvAxckee/p21XVoapaqaqV+ceUpPkYcnj7NuA/quq5qvof4F7g99rGkqQ2hpTe94G3JDk/SYDrgKNtY0lSG0Oe0zsC3AM8Cnx78j2HGueSpCYc9j0Kd890DvvW9jjsW5Kw9CR1xtKT1BVLT1JXLD1JXbH0JHVld6Of+2PgezNs/9rJ9yyrGfPvuLdjLPv+h+X/Hcw/X79xpi80eZ/erJKsLvM5u+Yf37L/DuZfHA9vJXXF0pPUlZ1Sest+Lq/5x7fsv4P5F2RHPKcnSYuyU1Z6krQQlp6kroxaekkOJPlOkmeS3DJmlrOR5OIkX01ydDIi86axM52NJLuSPJbkC2NnmVWSC5Pck+TpyZ/DW8fONItlHK+a5PYkJ5M8seGx1yR5MMm/TT6/esyMWxmt9JLsAj4JvAO4ErgxyZVj5TlLp4CPVNVvAW8BPrSEvwPATSzv1bBvBR6oqt8E3sQS/R5LPF7108CB0x67BfhyVV0OfHlyf0cac6V3DfBMVR2rqheBu1kfQLQ0qupEVT06uf0C6//g9o6bajZJ9gHvBA6PnWVWSV4F/D5wG0BVvVhVPx031cyWbrxqVT0M/OS0h68H7pjcvgN4z0JDzWDM0tsLHN9wf40lK4yNklwCXAUcGTfJzD4BfBT45dhBzsJlwHPApyaH54eTXDB2qKGGjlddEm+oqhOwvhgAXj9ynjMas/Q2OwF1Kd8/k+QVwOeAm6vq+bHzDJXkXcDJqnpk7CxnaTdwNfAPVXUV8F/s4MOq0w0dr6r5GrP01oCLN9zfxxIs7U+X5DzWC+/Oqrp37Dwz2g+8O8l3WX964doknxk30kzWgLXJ8CpYH2B19Yh5ZnUujVf9UZKLACafT46c54zGLL1vAJcnuTTJHtafwL1vxDwzm4zEvA04WlUfHzvPrKrqY1W1r6ouYX3/f6WqlmalUVU/BI4nuWLy0HXAUyNGmtW5NF71PuD9k9vvB/5pxCxbanVpqamq6lSSDwNfZP1Vq9ur6smx8pyl/cD7gG8neXzy2F9W1f0jZurNnwF3Tv7jPAZ8YOQ8g1XVkSQvjVc9BTzGEpzOleQu4A+A1yZZA/4a+Dvgs0k+yHqZ/8l4CbfmaWiSuuIZGZK6YulJ6oqlJ6krlp6krlh6krpi6UnqiqUnqSv/CxrxBzddfpf9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 1, 3,\n",
      "       1, 1, 3, 1, 1, 0, 0, 4, 0, 0, 4, 0, 0, 4, 3, 3, 3, 3, 3, 3, 0, 0,\n",
      "       0, 1, 1, 1, 4, 4, 4, 4, 4, 4]), <PIL.Image.Image image mode=RGBA size=12x9 at 0x208BC772748>)\n",
      "torch.Size([1, 3, 9, 12])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5 5 5 5 0 0 0 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 5 5 5 0 0 0 0 0 0 2 2 2 3\n",
      " 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 0 5 5 4 5 1 5 2 0 5 4 2 0 2 3 2 2 2 4 2 1 1 3 1 0 4 0 5 4 0 4 1 3 0 1\n",
      " 2 1 5 3 3 0 3 4 4 2 1 0 4 5 3 5 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "\n",
    "env.set_scramble(1, 2, False)\n",
    "env.reset()\n",
    "print(env.get_state())\n",
    "env.render()\n",
    "\n",
    "env.set_scramble(1, 2, False)\n",
    "env.reset()\n",
    "new_state, reward, is_done, _ = env.step(0)\n",
    "print(new_state)\n",
    "channels = state_to_chanels(new_state) # the alpha isn't interesting we are ignoring it\n",
    "print(channels.shape)\n",
    "env.render()\n",
    "\n",
    "env.set_scramble(1, 2, True)\n",
    "env.reset()\n",
    "print(env.get_state())\n",
    "env.render()\n",
    "\n",
    "env.set_scramble(20, 21, True)\n",
    "env.reset()\n",
    "print(env.get_state())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We use the larger DQN architecture from Mnih et al. (2015) consisting of 3 convolutional layers with 32, 64, and 64 channels followed by a hidden layer\n",
    "with 512 units. The convolutional layers use 8 × 8, 4 × 4, and 3 × 3 filters with strides of 4, 2, and 1, respectively. All hidden layers were followed by a rectifier nonlinearity (ReLU). The network contains over 4M parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 9, 12])\n"
     ]
    }
   ],
   "source": [
    "state = state_to_chanels(env.step(env.action_space.sample())[0])\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first try\n",
      "torch.Size([1, 3, 9, 12])\n",
      "[[129.7742    70.419876  62.36709    0.        32.410904 172.18762\n",
      "   18.609879   0.         0.       208.87338  205.04492    0.      ]]\n",
      "didn't change anything, should be the same\n",
      "[[129.7742    70.419876  62.36709    0.        32.410904 172.18762\n",
      "   18.609879   0.         0.       208.87338  205.04492    0.      ]]\n",
      "after evolution, should be different but only slightly\n",
      "[[134.04652   71.020386  61.650772   0.        15.223483 172.38501\n",
      "   12.838687   0.         0.       202.8052   203.65369    0.      ]]\n",
      "copied model, should be the same\n",
      "[[134.04652   71.020386  61.650772   0.        15.223483 172.38501\n",
      "   12.838687   0.         0.       202.8052   203.65369    0.      ]]\n"
     ]
    }
   ],
   "source": [
    "m = Model(random_state())\n",
    "print('first try')\n",
    "print(state.shape)\n",
    "print(to_np(m(state)))\n",
    "print('didn\\'t change anything, should be the same')\n",
    "print(to_np(m(state)))\n",
    "m.evolve(0.005, random_state())\n",
    "print('after evolution, should be different but only slightly')\n",
    "print(to_np(m(state)))\n",
    "\n",
    "m2 = uncompress_model(m.compress())\n",
    "print('copied model, should be the same')\n",
    "print(to_np(m2(state)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great, it compiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Explanation of Genetic Algorithms\n",
    "\n",
    "> A genetic algorithm (Holland, 1992; Eiben et al., 2003) evolves a population P of N individuals (here, neural network parameter vectors θ, often called genotypes). At every generation, each θi is evaluated, producing a fitness score (aka reward) F(θi). Our GA variant performs truncation selection, wherein the top T individuals become the parents of the next generation. To produce the next generation, the following process is repeated N-1 times: A parent is selected uniformly at random with replacement and is mutated by applying additive Gaussian noise to the parameter vector: θ' = θ + σε where ε ∼ N(0, I). The appropriate value of σ was determined empirically for each experiment, as described in Supplementary Information (SI) Table 3. The Nth individual is an unmodified copy of the best individual from the previous generation, a technique called elitism.\n",
    "\n",
    "This seems fairly clear to me, but I will attempt to re-explain it: basically instead of generating a network randomly and modifying its weights using gradient descent, we generate a bunch of networks randomly, constituting a \"generation\", then evaluate them. Surely some of them will perform slightly better. We pick the one that performs the very best and keep it for the next generation. Then among the top few networks we create the new generation by copying them a bunch of times and modifying the weights of each copy slightly. Because only the top networks get chosen each time and because we keep changing the networks, it seems like our performance should keep increasing, and indeed it often does!\n",
    "\n",
    "In our case, since we are focusing on Atari games, the population size is 5,000, the truncation threshold is 10 and the σ parameter is 0.005. The I parameter they mentioned simply means that it is sampled from a normal distribution with unit variance (I is the identity matrix).\n",
    "\n",
    "So, when do we stop with the generations, you might ask? Uber is very clear that they stopped after 1 billion **frames**, not after a fixed number of generations, so we will make sure to keep track of the number of frames that we see and stop once we've passed 1 billion. As a reminder, there is a difference between a frame and a forward pass on a neural network, since it is standard to run 4 frames for every forward pass (ie the network only chooses an action every 4 frames). If you use a `Deterministic-v4` in OpenAI gym, this frame skipping with 4 frames is automatically implemented for you, so simply multiply the number of times you called `.step` by 4, and stop when that number reaches a billion.\n",
    "\n",
    "## Model Compression\n",
    "\n",
    "One interesting innovation that Uber provides in this paper is in neural network compression.\n",
    "\n",
    ">We propose a novel method to store large parameter vectors compactly by representing each parameter vector as an initialization seed plus the list of random seeds that produce the series of mutations applied to θ. This innovation was essential to enabling GAs to work at the scale of deep neural networks, and we thus call it a Deep GA. This technique also has the benefit of offering a state-of-the-art compression method (Section 5).\n",
    "\n",
    "As the quote above implies, we absolutely **need** to implement this compression, not because it is cool but because we will need to have workers on several different machines, passing neural networks to each other, and it is simply crucial to be able to serialize the neural nets efficiently so that they can be transfered quickly from one machine to another.\n",
    "\n",
    "Fortunately, implementing this is very simple! As mentioned by the paper, it is simply about remembering the random seeds used each time we generated random numbers, ie we need to remember:\n",
    "* The seed we used for initializing the network\n",
    "* The seed we used every time we evolved the weights of the network.\n",
    "\n",
    "That's it! Whenever we pass the network around, we simply send the seeds instead of the actual network and it is easy to recreate it by re-running initialization and evolution after setting each of the seeds properly.\n",
    "\n",
    "The way to set a random seed using PyTorch is simply `torch.manual_seed`, to which you can pass a number. To make sure the random seeds were always different, I passed in a number generated by Python's `random.randint`, sampled from 0 to 2^31 - 1 (why this number you ask? Well I wasn't sure what numbers were allowable as random seeds, but this makes sure whatever I pass fits in a 32 bit signed integer, which I felt was almost certainly acceptable, while bigger numbers might not be).\n",
    "\n",
    "**Learning: use random SEEDS, not random STATES.**\n",
    "\n",
    "PyTorch also has a way to get its current random **state** using `torch.get_rng_state`, and to set it using `torch.set_rng_state`. At first I used this because there are a lot more possible random states than random seeds, so it seemed more \"random\" to me. The big problem is that each random state is already around 5kB in size (by contrast, a random seed is only 4 bytes, or 1,000 times less), which meant that my system quickly became very slow due to the large amount of data involved. Don't make the same mistake I did!\n",
    "\n",
    "\n",
    "## Distributed Computing: Job Queues and Spot Instances\n",
    "\n",
    "**Fair warning**: in this section I will assume you already know a bit about AWS or some other cloud provider like Google Cloud or Microsoft Azure. If you don't, it might be worthwhile to at least take a look at some tutorial for using AWS for deep learning (eg: https://towardsdatascience.com/how-to-set-up-a-deep-learning-environment-on-aws-with-keras-theano-b0f39e3d861c), or at least just be aware that you'll likely have to do some Googling because I will skip over some important basic things. \n",
    "\n",
    "Training a neural network using genetic algorithms is different from training it using gradient descent in the following way: with gradient descent we need to do a lot of math on a single network, while with a genetic algorithm we need to do much less math (just a forward pass, not forward and backward) on many networks. This makes it possible to train our GA neural network on a massive number of CPUs instead of on single GPUs. \n",
    "\n",
    "How massive are we talking though? Let's see what Uber has to say about that:\n",
    "\n",
    "> The GA and ES are both faster than Q-learning and policy gradient methods when substantial distributed computation is available (here, 720 CPU cores across dozens of machines).\n",
    "\n",
    "**720**? Wow. I tend to think my own hardware is fairly high-end and up to date, but even between my desktop and my laptop I have a grand total of 8 CPU cores...\n",
    "\n",
    "So is this another one of these papers that use an amount of resources completely unaccessible to us normal mortals? Not so! We're just going to need to learn a bit more about infrastructure.\n",
    "\n",
    "### Job Queues\n",
    "\n",
    "Here's the thing, there is absolutely no way we're going to get 720 CPU cores on a single machine. More reasonably we can get 64 or a bit more. This means that if we want to process things as far as Uber did, we are going to need multiple machines.\n",
    "\n",
    "For this, we are going to use the concept of a task queue, with a master (or manager) and workers. The master will put tasks onto the task queue, and the workers will take jobs from the queue, run them, and write the results back. The process can be seen in this diagram.\n",
    "\n",
    "![Source: https://www.alberton.info/batching_vs_latency_and_jobqueue_models.html](https://www.alberton.info/images/articles/jobqueues/jobqueue1.png)\n",
    "\n",
    "In our case, the jobs are evaluating the performance of the different neural networks in the current generation, and the outputs are simply the scores of each neural networks, as well as the number of frames they used. Another advantage of this architecture is that it allows us to put the master and the workers on different machines, which is useful for keeping costs low: we can put the master on a not very powerful but very reliable machine, while we can put the workers on machines that are very powerful but not as reliable. This way we get to keep our data (since everything is stored on the master), but can use unreliable cheap machines as workers.\n",
    "\n",
    "The task queue library that we will use is [RQ](http://python-rq.org), which is a very easy to use library for task queues. It relies on the Redis database, which you can install quite simply on an Amazon instance.\n",
    "\n",
    "All you need to do is to make sure that *all* the code necessary for your workers is in a `.py` file, not in a notebook, as RQ needs `.py` files to create worker processes. Your master can be in a notebook, however. You can import whatever parts of your `.py` you need in your master notebook.\n",
    "\n",
    "Specifically the following should be in a `.py` file:\n",
    "* The code for the neural network model\n",
    "* The code for neural network compression/decompression\n",
    "* A function for evaluating a neural network from a compressed model and an environment name that returns at least the score and the number of frames used\n",
    "\n",
    "Your master code can then add a job to the queue using `job = rq.enqueue(...)` with the function in question and the arguments it should take, and it can letter get the result using `job.result`, which will be None if the function hasn't returned yet. You should make sure that your code is robust to jobs being dropped: if you don't get a result for a job for a long time, make sure it is being reenqued. Remember to pass **compressed** neural nets as arguments to `enqueue`, since these will need to be serialized and transfered over the network, it is important that they be small.\n",
    "\n",
    "Once you have all of this, you should create an AWS instance for your master, a small-ish one is sufficient, personally I used a t2.medium. You can install any Linux you want on it, personally I used the non-deep learning Ubuntu 16.04 AMI. Then install all the dependencies for your agent, including redis-server, rq, pytorch etc. Once this is done, you should do the following things:\n",
    "\n",
    "* Edit the security group of the master machine you just created to allow for redis connections. To make sure you don't expose your redis server to the world, you can limit the traffic to just that coming from inside the same security group. Starting now, make sure any other machine you create is in the same security group. Ideally you'd only enable redis traffic, but I was too lazy to find out exactly how, so I enabled all traffic from the security group like so:\n",
    "![image.png](aws_security.png)\n",
    "* Find out the private IP of your master machine (should be in the EC2 web interface) and create the following two files:\n",
    " * A file named `redis.conf` containing `bind <YOUR PRIVATE IP>`\n",
    " * A file named `settings.py` containing `REDIS_HOST = '<YOUR PRIVATE IP'`\n",
    "* Run redis server: `nohup redis-server redis.conf&`. The `nohup` part makes sure the server will keep running even if your connection dies and the `&` at the end makes sure it runs in the background right away so you can do other stuff.\n",
    "* Run an RQ worker using `rq worker -c settings`.\n",
    "* Now **test** that this works by running your master code and making sure the jobs are indeed processed by the worker.\n",
    "* Kill the worker, go back to the AWS interface, click on your instance and go to \"Actions -> Images -> Create Image\", this will allow you to create an AMI containing all the packages that you just installed, so that it will be easy to launch workers on other machines!\n",
    "\n",
    "We can now go on to create the actual workers.\n",
    "\n",
    "### AWS Spot Instances\n",
    "\n",
    "As it turns out, getting access to lots of CPUs is fairly easy: the c5.18xlarge instance on AWS, for instance, contains a staggering 72 CPUs, 1/10th of what we need! With just 10 of these instances we'll have exactly the same amount of CPUs that Uber used. There is even an instance with 96 CPUs (the m5.24xlarge), but it is worth noting that its CPUs are slightly slower than the c5.18xlarge and its cost per CPU is slightly higher due to the fact that it also has a lot more memory, which we don't really need (the c5.18xlarge has 144 GB of memory and the m5.24xlarge has 384 GB, but as I recall I never used more than 5 GB on any machine I used...).\n",
    "\n",
    "Ok so how much will it cost? Well, a single c5.18xlarge costs \\$3.06 per hour in the US-West region (other regions should be in the same ballpark), and Uber claims that they were able to train their networks in about an hour using the equivalent of 10 c5.18xlarge, so training a single network should cost us \\$30.60, assuming all goes smoothly. This is actually not so bad! But we can do much better using **spot instances**.\n",
    "\n",
    "Spot instances are how Amazon deals with the fact that not all of their machines are reserved at any given time: they set up an auction for usage of the idle machines, and the highest bidder gets to access them. This makes for much lower prices, but there is a catch: the machine you are using may be taken away from you if someone beats your maximum bid or if Amazon has to fulfill a full-price request.\n",
    "\n",
    "Additionally, the price you actually pay can be less predictable than with regular on-demand instances: the spot price of instances fluctuates as demand increases and decreases, and you are always paying the current spot price as long as it is lower than your maximum bid. So if an instance is trading at \\$1 an hour and you bid \\$2, you will probably pay \\$1 at first but over time you might start paying \\$1.50 an hour, or \\$0.50 etc. Fortunately, the market for instances is stable enough now that this is not a big concern: if you are only using your instances for a few hours, most likely you will pay within 5% or so of the price you saw when you first created the spot instance.\n",
    "\n",
    "Overall though, the design we described above is perfect for spot instances as you can have a \"master\" server running on a small on-demand instance and use however many high-CPU spot instances you want. If you wrote your code properly, it should be relatively immune to spot instances dying: it will just retry jobs and not lose data. As of this writing, a c5.18xlarge spot instance costs \\$1.08, so almost 3x less than using an on-demand instance. This means that, optimistically, it should cost around \\$11 to train on a single game. Note, however, that due to the fact that I trained on 3 games (Frostbite, Breakout and Space Invaders) and due to the many errors I made along the way, this whole experiment cost me around \\$115, so try to be extra careful or make sure you're willing to spend around \\$100 before you start doing this.\n",
    "\n",
    "So how do you create a spot instance? Let's do that now! As a reminder, please make sure you are reasonably confident about your setup (ie you have created an AMI and you have confirmed that you can easily start running the system on another machine) before you do this.\n",
    "\n",
    "In the EC2 panel, click on \"Spot Requests\" on the right and then on the \"Request Spot Instances\" button. You will be taken to this page:\n",
    "![image.png](spot.png)\n",
    "\n",
    "Many of the parameters you can keep as default, just remember to do the following:\n",
    "* Select your total capacity (10 would give you what Uber used, but I personally chose to use 5 instances and simply wait for 2 hours instead of 1. You'll have to ssh into every machine so a lower number is more manageable).\n",
    "* Select the AMI you just created.\n",
    "* Remove the c3.large instance type from the instance type list, and add a c5.18xlarge instead. The select instance type window will actually show you all the instances that are available and their prices.\n",
    " * It is possible that another instance type will be more advantageous for you when you look. My advice when figuring out what instance is best is simply to divide the current spot price by the number of CPUs to find the cheapest instance per CPU. Note, however, that the instances whose name start with a \"c\" are compute-optimized, which typically means that their CPUs are faster than those on other instances, even when the number of cores is equal.\n",
    "* Select the same availability zone as your master (maybe not necessary).\n",
    "* Select the security group you previously created/edited.\n",
    "\n",
    "Some optional things are to set your maximum price (the default is to bid the On-Demand price, which is what I would recommend anyway) and maybe to reserve your spot instance for a fixed duration (this way you are guaranteed that your instance won't be preempted for the next, say, 3 hours, but you might be paying more than with an ordinary spot instance).\n",
    "\n",
    "Once this is done, you can log into the various spot instances and run the rq workers on them. This is the script I used to start 72 workers and have each write its log into a different file:\n",
    "\n",
    "```\n",
    "for i in {0..72}\n",
    "do\n",
    "nohup rq worker -c settings > $i&\n",
    "done\n",
    "```\n",
    "\n",
    "Now, assuming you are running your master code on the master box, you should finally be making progress with your training!\n",
    "\n",
    "### An Important Aside on Intel MKL\n",
    "\n",
    "A very cool recent development in numerical computing is Intel MKL. This is an extremely fast math library developed by Intel which takes advantage of recent instructions and multithreading to perform numerical computations very quickly. It is used by PyTorch for CPU computations and helps reduce the difference between CPU and GPU performance for neural networks (though GPUs are still faster).\n",
    "\n",
    "Unfortunately, MKL will cause problems for us! This is because it will automatically use several CPU cores for the forward pass of a single neural network, but we already set things up so that we use one CPU core per neural network. I found that this seemed to make things slower for this purpose, which is why I recommend disabling multithreading in MKL. This can be done by installing mkl-service in python (`conda install mkl-service`) and putting the following lines at the top of your worker file, before you import pytorch:\n",
    "\n",
    "```\n",
    "import mkl\n",
    "mkl.set_num_threads(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_HOST = '10.156.0.8'  # TODO: you absolutely need to change this if you use this code.\n",
    "REDIS_PORT = 6380\n",
    "\n",
    "class FakeJob:\n",
    "    def __init__(self, j):\n",
    "        self.result = j.result\n",
    "\n",
    "class GA:\n",
    "    def __init__(self, population, compressed_models=None, queue_name='default'):\n",
    "        self.population = population\n",
    "        self.models = [CompressedModel() for _ in range(population)] if compressed_models is None else compressed_models\n",
    "        \n",
    "        self.redis = Redis(REDIS_HOST, REDIS_PORT)\n",
    "        self.queue = Queue(connection=self.redis, name=queue_name)\n",
    "        for j in self.queue.jobs:\n",
    "            j.cancel()\n",
    "\n",
    "    # Note: the paper says \"20k frames\", but there are 4 frames per network\n",
    "    # evaluation, so we cap at 5k evaluations\n",
    "    def get_best_models(self, env, logger, max_eval=5000):\n",
    "        jobs = []\n",
    "        for m in self.models:\n",
    "            jobs.append(self.queue.enqueue(evaluate_model, gym.make(env), m, logger, max_eval=max_eval, ttl=650, job_timeout=600))\n",
    "        last_enqueue_time = time.time()\n",
    "        while True:\n",
    "            for i in range(len(jobs)):\n",
    "                if jobs[i].result is not None and not isinstance(jobs[i], FakeJob):\n",
    "#                     if random.random() < 0.001:\n",
    "                    printMe(logger, f'total reward - {jobs[i].result[0]}, is succeeded - {jobs[i].result[2]}')\n",
    "                    jobs[i] = FakeJob(jobs[i])\n",
    "                    \n",
    "            def convert_result(j):\n",
    "                if j.result is not None:\n",
    "                    if j.result[0] == 0.0 and j.result[1] == max_eval * 4 and 'Breakout' in env:\n",
    "                        return -1.0\n",
    "                    return j.result[0]\n",
    "                return None\n",
    "            scores = [convert_result(j) for j in jobs]\n",
    "            if None not in scores:\n",
    "                break\n",
    "            if time.time() - last_enqueue_time > 600:\n",
    "                printMe(logger, f'Reenqueuing unfinished jobs ({sum(x is None for x in scores)}).')\n",
    "                for i in range(len(jobs)):\n",
    "                    if jobs[i].result is None:\n",
    "                        jobs[i].cancel()\n",
    "                        jobs[i] = self.queue.enqueue(\n",
    "                            evaluate_model, gym.make(env), self.models[i], logger, max_eval=max_eval, ttl=650, job_timeout=600)\n",
    "                last_enqueue_time = time.time()\n",
    "            time.sleep(1)\n",
    "        used_frames = sum(j.result[1] for j in jobs)\n",
    "        scored_models = list(zip(self.models, scores))\n",
    "        scored_models.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        complete_proportion = np.count_nonzero(j.result[2] for j in jobs)\n",
    "        \n",
    "        return scored_models, used_frames, complete_proportion \n",
    "\n",
    "    def evolve_iter(self, env, logger, sigma=0.005, truncation=10, max_eval=5000):\n",
    "        scored_models, used_frames, complete_proportion = self.get_best_models(env, logger, max_eval=max_eval)\n",
    "        scores = [s for _, s in scored_models]\n",
    "        median_score = np.nanmedian(scores)\n",
    "        mean_score = np.nanmean(scores)\n",
    "        max_score = scored_models[0][1]\n",
    "        scored_models = scored_models[:truncation - 1]\n",
    "        scored_models.append((CompressedModel(), 0))\n",
    "        \n",
    "        # Elitism\n",
    "        self.models = [m for m, _ in scored_models]\n",
    "        for _ in range(self.population - truncation):\n",
    "            self.models.append(copy.deepcopy(random.choice(scored_models)[0]))\n",
    "            self.models[-1].evolve(sigma)\n",
    "            \n",
    "        return median_score, mean_score, max_score, used_frames, complete_proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(env, model, max_eval=200000):\n",
    "    env = gym.make(env)\n",
    "    model = uncompress_model(model)\n",
    "    cur_state = state_to_chanels(env.reset())\n",
    "    total_reward = 0\n",
    "    net_reward = 0\n",
    "    frames = [env.render()]\n",
    "\n",
    "    model.eval()\n",
    "    for _ in range(max_eval):\n",
    "        values = model(cur_state)[0]\n",
    "        net_reward += values.max()/values.sum()\n",
    "        action = values.argmax()\n",
    "        new_state, reward, is_done, _ = env.step(action.item())\n",
    "        frames.append(env.render())\n",
    "        if is_done:\n",
    "            break\n",
    "        cur_state = state_to_chanels(new_state)\n",
    "\n",
    "    net_reward += reward         \n",
    "    fitness = (torch.Tensor(random.uniform(0.0, 0.3)) \n",
    "                if reward != 1 \n",
    "                else (net_reward/(env.get_step_count() + 1)) + (1 - (env.get_step_count()/env.max_steps)))\n",
    "    return fitness.item(), frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env,  logger, do_run=True, render_vids=True):\n",
    "    if do_run:\n",
    "        ga = GA(5000)\n",
    "\n",
    "        total_frames = 0\n",
    "        all_results = [(0.0, 0.0, 0.0, 0, time.time())]\n",
    "        while total_frames < 250_000_000:\n",
    "            if 'Breakout' in env.spec.id and total_frames < 2_500_000:\n",
    "                med, avg, M, frames, complete_proportion = ga.evolve_iter(env.spec.id, logger, max_eval=400)\n",
    "            else:\n",
    "                med, avg, M, frames, complete_proportion = ga.evolve_iter(env.spec.id, logger)\n",
    "            total_frames += frames\n",
    "            all_results.append((med, avg, M, frames, time.time()))\n",
    "            printMe(logger, f'Done with generation!\\nMedian: {med}, average: {avg}, max: {M}, frames: {total_frames:,}, complete: {complete_proportion}')\n",
    "        best_models = ga.get_best_models(env.spec.id, logger)[0]\n",
    "        pickle.dump(all_results, open(f'{env.spec.id}_process.pickle', 'wb'))\n",
    "        pickle.dump(best_models, open(f'{env.spec.id}_best.pickle', 'wb'))\n",
    "    \n",
    "    best_models = pickle.load(open(f'{env.spec.id}_best.pickle', 'rb'))\n",
    "    process = pickle.load(open(f'{env.spec.id}_process.pickle', 'rb'))\n",
    "    \n",
    "    if render_vids:\n",
    "        videos = [make_video(env.spec.id, best_models[0][0]) for _ in range(30)]\n",
    "        pickle.dump(videos, open(f'{env.spec.id}_videos.pickle', 'wb'))\n",
    "    videos = pickle.load(open(f'{env.spec.id}_videos.pickle', 'rb'))\n",
    "    \n",
    "    def convert_vid(video):\n",
    "        res = []\n",
    "        for v in video:\n",
    "            img = PIL.Image.fromarray(v)\n",
    "            img = img.resize((320, 240))\n",
    "            res.append(np.array(img))\n",
    "        return res\n",
    "    \n",
    "    videos = [(s, convert_vid(v)) for s, v in videos]\n",
    "    videos.sort(key=lambda x:x[0])\n",
    "    best_score, best_vid = videos[-1]\n",
    "    worst_score, worst_vid = videos[0]\n",
    "    median_score, median_vid = videos[len(videos) // 2]\n",
    "    printMe(logger, f'Best score: {best_score}')\n",
    "    gif_file = f'{env.spec.id}_best.gif'\n",
    "    imageio.mimsave(gif_file, best_vid + [best_vid[-1]] * 10, fps=20)\n",
    "    display(HTML(f'<img src=\"{gif_file}\">'))\n",
    "\n",
    "    printMe(logger, f'Worst score: {worst_score}')\n",
    "    gif_file = f'{env.spec.id}_worst.gif'\n",
    "    imageio.mimsave(gif_file, worst_vid + [worst_vid[-1]] * 10, fps=20)\n",
    "    display(HTML(f'<img src=\"{gif_file}\">'))\n",
    "\n",
    "    printMe(logger, f'Median score: {median_score}')\n",
    "    gif_file = f'{env.spec.id}_median.gif'\n",
    "    imageio.mimsave(gif_file, median_vid + [median_vid[-1]] * 10, fps=20)\n",
    "    display(HTML(f'<img src=\"{gif_file}\">'))\n",
    "\n",
    "    for idx, name in enumerate(['Median Score', 'Average Score', 'Max Score', 'Frames Per Run', 'Total Time']):\n",
    "        if name == 'Frames':\n",
    "            div = 5000\n",
    "        else:\n",
    "            div = 1\n",
    "        sb.set_style(\"darkgrid\")\n",
    "        plt.plot(list(range(1, len(process))), [(p[idx] - process[0][idx]) / div for p in process[1:]])\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel(name)\n",
    "        plt.savefig(f'{env.spec.id}_{name}.png')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the results!\n",
    "\n",
    "Finally, it's one or two hours later, depending on how many machines you chose to use, and we have results!\n",
    "\n",
    "So how do we evaluate the performance of our agents? Well, Uber has a few things to say about that:\n",
    "> Comparing our results with those from other algorithms fairly is extremely difficult, as such comparisons are inherently apples and oranges in many different ways. One important consideration is whether agents are evaluated on random starts (a random number of no-op actions), which is the regime they are trained on, or starts randomly sampled from human play, which tests for generalization (Nair et al., 2015). **Because we do not have a database of human starts to sample from**, our agents are evaluated with random starts. Where possible, we compare our results to those for other algorithms for which such random start results are available. That is true for DQN and ES, but not true for A3C, where we had to include results on human starts.\n",
    "\n",
    "The emphasis on lacking a database of human starts is mine: I think this is too bad, and indeed there does not seem to be such a database and I think it would be great to build one.\n",
    "\n",
    "Regardless, it sounds like the way we should evaluate our agent is simply using a random number of no-op operations before starting them, which is how we did training in the first place. The number of no-ops is not specified here but it is given as between 0 and 30 elsewhere, so that's what we'll do. Because there is this randomness aspect (not to mention the potential for random number generation within the individual Atari games), I decided to run the game 30 times and to display the best, worst, and median result. Here goes, for Frostbite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger():\n",
    "    logger = logging.getLogger()\n",
    "    fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error 10060 connecting to 10.156.0.8:6380. A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\redis\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m             \u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\redis\\connection.py\u001b[0m in \u001b[0;36m_connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"socket.getaddrinfo returned an empty list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\redis\\connection.py\u001b[0m in \u001b[0;36m_connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    602\u001b[0m                 \u001b[1;31m# connect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-972c5a528fd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrun_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprintMe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'game time - {time.time() - start_time}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-020cecab92f1>\u001b[0m in \u001b[0;36mrun_env\u001b[1;34m(env, logger, do_run, render_vids)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender_vids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdo_run\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mga\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-3fab16fd9d1d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, population, compressed_models, queue_name)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mredis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRedis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREDIS_HOST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mREDIS_PORT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQueue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mredis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rq\\queue.py\u001b[0m in \u001b[0;36mjobs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;34m\"\"\"Returns a list of all (valid) jobs in the queue.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rq\\queue.py\u001b[0m in \u001b[0;36mget_jobs\u001b[1;34m(self, offset, length)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;34m\"\"\"Returns a slice of jobs in the queue.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mjob_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_job_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcompact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjob_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjob_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rq\\queue.py\u001b[0m in \u001b[0;36mget_job_ids\u001b[1;34m(self, offset, length)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         return [as_text(job_id) for job_id in\n\u001b[1;32m--> 164\u001b[1;33m                 self.connection.lrange(self.key, start, end)]\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\redis\\client.py\u001b[0m in \u001b[0;36mlrange\u001b[1;34m(self, name, start, end)\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mPython\u001b[0m \u001b[0mslicing\u001b[0m \u001b[0mnotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m         \"\"\"\n\u001b[1;32m-> 1975\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LRANGE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlrem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\redis\\client.py\u001b[0m in \u001b[0;36mexecute_command\u001b[1;34m(self, *args, **options)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection_pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mcommand_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\redis\\connection.py\u001b[0m in \u001b[0;36mget_connection\u001b[1;34m(self, command_name, *keys, **options)\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m             \u001b[1;31m# ensure this connection is connected to Redis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m             \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m             \u001b[1;31m# connections that the pool provides should be ready to send\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m             \u001b[1;31m# a command. if not, the connection was either returned to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\redis\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Timeout connecting to server\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_error_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: Error 10060 connecting to 10.156.0.8:6380. A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond."
     ]
    }
   ],
   "source": [
    "env = gym.make('RubiksCube-v0')\n",
    "env.set_scramble(20, 20, True)\n",
    "start_time = time.time()\n",
    "logger = set_logger()\n",
    "run_env(env, logger, True, True)\n",
    "printMe(logger, f'game time - {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frostbite\n",
    "\n",
    "We can see that we get an agent that performs very well for Frostbite: 4,570 as its best score, which is close to Uber's 4,801, and which is roughly the state of the art right now as far as I know! Very exciting.\n",
    "\n",
    "Unfortunately, as we can see, the agent is not at all robust even to random starts! The median score is only 170 and the worst score is 160! It isn't clear to me why Uber did not report this: is it an issue in my implementation? Something that happened to them as well but that they failed to report? An issue in their implementation? (if they implemented random starts badly for instance, this might not be visible). \n",
    "\n",
    "This seems to explain the extremely large discrepancy between the median score and the best score as I go through generations (Uber's graph of \"median\" scores are not using this definition, which is why I can't really compare them to what I found: in Uber's definition it is the median of the best agent across multiple runs): it is not that agents are very sensitive to small changes in their weights, it is that they are very sensitive to small changes in their starting point!\n",
    "\n",
    "I am really interested in fixing this issue in the future. In my mind, there are two things that could help fix this:\n",
    "* Evaluating each neural net not on a single run but on several, each of which start with a different number of no-ops.\n",
    "* Revisiting epsilon greedy for genetic algorithms: this seems crazy because it doesn't seem like GA \"need\" an exploration strategy, which is why Uber didn't implement one, but it could be that exploration doesn't just help reinforcement learning agents only with finding new high-reward states, but also with becoming robust to finding themselves in unforseen situations. Right now it seems like the GA networks are overfitting to a very specific sequence of actions from the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_env(gym.make('BreakoutDeterministic-v4'), False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout\n",
    "\n",
    "Oh boy... GAs do **terribly** on Breakout. Why?\n",
    "\n",
    "I can see a few reasons:\n",
    "* You need to run a specific action or breakout won't start (specifically, breakout has 4 actions: do nothing, start the game, move left and move right).\n",
    " * This is not a problem when following a randomized exploration strategy such as epsilon-greedy, because the action will get chosen at some point or other, but without any exploration at all, it means that a large proportion of agents will simply do nothing forever.\n",
    " * To prevent this, I even made sure to reduce the maximum length of a game in the first few generations and to penalize agents that had done nothing at all the whole time. Obviously it didn't work that well, however.\n",
    "* Very few of the pixels change in breakout, so agents keep performing the same action.\n",
    " * In Frostbite, almost 50% of the pixels are guaranteed to change even if the agent does nothing. In Breakout, only a tiny ball, the paddle, and maybe a brick or two will change, and only if the agent actually does something. For a randomly initialized neural network, this is likely not enough for it to decide to perform a different action.\n",
    " \n",
    "Again, I think there are ways to fix this issue, including again including epsilon-greedy exploration in training. This warrants further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_env(gym.make('SpaceInvadersDeterministic-v4'), False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Invaders\n",
    "\n",
    "The agent does relatively well on space invaders, similar to the agent we trained in https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c, which is far from state of the art, but still pretty decent. Besides, it actually performs the same regardless of the initial random no-ops.\n",
    "\n",
    "However, I am a bit concered about its strategy which consists, at least at first, in simply staying far to the left and trying to shoot the mothership: this looks more like a weird local optimum than an actual skill that the agent has obtained. Again I am curious what would make it learn something that looks more like real skill at the game and not a lazy strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I am excited about the results in Uber's paper. The fact that it does so well at Frostbite is quite amazing, and it is unclear to me if this means that traditional RL algorithms are particularly weak or that GA is surprisingly strong. I think, however, that there seem to be significant issues with GA that Uber doesn't point out, partly because they weren't able to evaluate their algorithms using a human-start database (which I think would have shown the brittleness of the various agents), and it would be interesting to try to fix these. I for one plan to try including epsilon-greedy exploration in the future, which I think will help with the robustness of the agents, and as Uber points out, there is a ton of literature available on genetic algorithms, and they only tried the most basic possible ones. Who knows where more advanced GAs will take us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
